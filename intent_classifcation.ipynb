{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('intent_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Howdy</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good bye</td>\n",
       "      <td>goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how are things?</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We’d like a table for 5, please.</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Could you make me some tea?\\n</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           sentence    intent\n",
       "0                             Howdy  greeting\n",
       "1                          good bye   goodbye\n",
       "2                   how are things?  greeting\n",
       "3  We’d like a table for 5, please.     order\n",
       "4     Could you make me some tea?\\n     order"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using cosine similarity to identify intent from a  given text\n",
    "transform = tft.transform(data['sentence'])\n",
    "def check(text):\n",
    "    vect= []\n",
    "    for items in transform:\n",
    "        result = cosine_similarity(text,items)\n",
    "        vect.append(result)\n",
    "    ind = np.argmax(vect)\n",
    "    return ind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intent is:: greeting\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "label = list(data['intent'])\n",
    "tft = TfidfVectorizer()\n",
    "tft.fit(data['sentence'])\n",
    "\n",
    "test2 = [input('Enter text')]\n",
    "te1 = tft.transform(test2)\n",
    "index = check(te1)\n",
    "print(\"The intent is::\",label[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "# Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard function which gives the jaccard similarity in percentage and appended in a label and maximum \n",
    "#percentage index is returned\n",
    "def jaccard(text):\n",
    "    \n",
    "    label = []\n",
    "    for items in b:\n",
    "#         print(items)\n",
    "        \n",
    "        inter = len(items.intersection(text))\n",
    "        union = len(items.union(text))\n",
    "        jaccard = float(inter/union)*100\n",
    "        \n",
    "        label.append(jaccard)\n",
    "        \n",
    "    return np.argmax(label)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function lemmatize the given strings and remove the puncutation and remove the stopwords\n",
    "def preprocess(items):\n",
    "    li = []\n",
    "    \n",
    "    lemmas = [ wordnet_lemmatizer.lemmatize(items, pos='v')  ]\n",
    "    text = ''.join(lemmas) \n",
    "\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "\n",
    "    word =  [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    word = ' '.join(word)\n",
    "    \n",
    "    return word\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling preprocess function to get the processed sentence list\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "    \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nopun = data['sentence'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       Howdy\n",
       "1                    good bye\n",
       "2                      things\n",
       "3    We’d like table 5 please\n",
       "4              Could make tea\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since jaccard similarity only take set as input and every word should be separated so splitting \n",
    "#the data and making set of it\n",
    "b = []\n",
    "for filename in nopun:\n",
    "#     print(type(filename))\n",
    "    a = filename.split()\n",
    "    b.append(set(a))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the text howdy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intent of this sentence using jaccard similarity is :: greeting\n"
     ]
    }
   ],
   "source": [
    "#Taking input from user and and splitting it and making set of it as this format is expexted \n",
    "#by the jaccard function\n",
    "inputt = input('Enter the text')\n",
    "sep = inputt.split()\n",
    "sep = set(sep)\n",
    "# tex = {'Howdy','good','things','bye'}\n",
    "c = jaccard(sep)\n",
    "label = list(data['intent'])\n",
    "print('The intent of this sentence using jaccard similarity is ::',label[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tfid vectorization to vectorize the given text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['sentence'], data['intent'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TfidfVectorizer()\n",
    "tft.fit(X_train)\n",
    "# feat.get_feature_names()\n",
    "trans = tft.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using naive bayes classifer to find the probablity of each text and identify its intent\n",
    "model = MultinomialNB(alpha=0.1)\n",
    "mod = model.fit(trans,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(tft.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     goodbye       0.67      0.33      0.44         6\n",
      "    greeting       0.75      0.94      0.83        16\n",
      "       order       0.92      0.86      0.89        14\n",
      "\n",
      "   micro avg       0.81      0.81      0.81        36\n",
      "   macro avg       0.78      0.71      0.72        36\n",
      "weighted avg       0.80      0.81      0.79        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
